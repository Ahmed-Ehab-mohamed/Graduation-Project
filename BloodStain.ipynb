{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHxt0TKaglMQ",
        "outputId": "a6d5bc50-c416-42c7-a8ff-f40882c9dfd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spectral in /usr/local/lib/python3.10/dist-packages (0.23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install spectral\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive, files\n",
        "import spectral.io.envi as envi\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QX5pMh5iRAJ",
        "outputId": "982289ba-91cf-40ae-982f-f955485d37bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "#change this to your DS location\n",
        "PATH_DATA = '/content/drive/MyDrive/ComputerVision/HyperBlood/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeaXamb6g4sP",
        "outputId": "5457ff4f-ada1-4334-9b1b-2051efc64865"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES = ['B(1)','C(1)','D(1)','E(1)','E(7)','E(21)','F(1)','F(1a)','F(1s)','F(2)','F(2k)','F(7)','F(21)']\n"
      ],
      "metadata": {
        "id": "77WWZMBBg6uk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(name,remove_bands=True,clean=True):\n",
        "    \"\"\"\n",
        "    Returns HSI data from a datacube\n",
        "\n",
        "    Parameters:\n",
        "    ---------------------\n",
        "    name: name\n",
        "    remove_bands: if True, noisy bands are removed (leaving 113 bands)\n",
        "    clean: if True, remove damaged line\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    data, wavelenghts as numpy arrays (float32)\n",
        "    \"\"\"\n",
        "    name = convert_name(name)\n",
        "    filename = \"{}data/{}\".format(PATH_DATA,name)\n",
        "    hsimage = envi.open('{}.hdr'.format(filename),'{}.float'.format(filename))\n",
        "    wavs = np.asarray(hsimage.bands.centers)\n",
        "    data = np.asarray(hsimage[:,:,:],dtype=np.float32)\n",
        "\n",
        "    #removal of damaged sensor line\n",
        "    if clean and name!='F_2k':\n",
        "        data = np.delete(data,445,0)\n",
        "\n",
        "\n",
        "    if not remove_bands:\n",
        "        return data,wavs\n",
        "    return data[:,:,get_good_indices(name)],wavs[get_good_indices(name)]"
      ],
      "metadata": {
        "id": "IpNSUJaohF8b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anno(name,remove_uncertain_blood=True,clean=True, path=PATH_DATA):\n",
        "    \"\"\"\n",
        "    Returns annotation (GT) for data files as 2D int numpy array\n",
        "    Classes:\n",
        "    0 - background\n",
        "    1 - blood\n",
        "    2 - ketchup\n",
        "    3 - artificial blood\n",
        "    4 - beetroot juice\n",
        "    5 - poster paint\n",
        "    6 - tomato concentrate\n",
        "    7 - acrtylic paint\n",
        "    8 - uncertain blood\n",
        "\n",
        "    Parameters:\n",
        "    ---------------------\n",
        "    name: name\n",
        "    clean: if True, remove damaged line\n",
        "    remove_uncertain_blood: if True, removes class 8\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    annotation as numpy 2D array\n",
        "    \"\"\"\n",
        "    name = convert_name(name)\n",
        "    filename = \"{}anno/{}\".format(path,name)\n",
        "    anno = np.load(filename+'.npz')['gt']\n",
        "    #removal of damaged sensor line\n",
        "    if clean and name!='F_2k':\n",
        "        anno = np.delete(anno,445,0)\n",
        "    #remove uncertain blood + technical classes\n",
        "    if remove_uncertain_blood:\n",
        "        anno[anno>7]=0\n",
        "    else:\n",
        "        anno[anno>8]=0\n",
        "\n",
        "    return anno"
      ],
      "metadata": {
        "id": "pSdm3FHuhN0b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_good_indices(name=None):\n",
        "    \"\"\"\n",
        "    Returns indices of bands which are not noisy\n",
        "\n",
        "    Parameters:\n",
        "    ---------------------\n",
        "    name: name\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    numpy array of good indices\n",
        "    \"\"\"\n",
        "    name = convert_name(name)\n",
        "    if name!='F_2k':\n",
        "        indices = np.arange(128)\n",
        "        indices = indices[5:-7]\n",
        "    else:\n",
        "        indices = np.arange(116)\n",
        "    indices=np.delete(indices,[43,44,45])\n",
        "    return indices"
      ],
      "metadata": {
        "id": "YbDiuMvehTrq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_name(name):\n",
        "    \"\"\"\n",
        "    Ensures that the name is in the filename format\n",
        "    Parameters:\n",
        "    ---------------------\n",
        "    name: name\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    cleaned name\n",
        "    \"\"\"\n",
        "    name = name.replace('(','_')\n",
        "    name = name.replace(')','')\n",
        "    return name"
      ],
      "metadata": {
        "id": "BGggXREOhn4M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, desired_width=224, desired_height=224):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Resize the image to match the input size expected by your models\n",
        "    image = image.resize((desired_width, desired_height))\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    # Normalize pixel values to be in the range [0, 1]\n",
        "    normalized_image = image_array / 255.0\n",
        "\n",
        "    # Add any additional preprocessing steps based on your model requirements\n",
        "\n",
        "    return normalized_image"
      ],
      "metadata": {
        "id": "4m8aIT-A8xPE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "r35xJmX8hcOh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uTKZ-mtioQ1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    pca = PCA(n_components=10)  # Reduce to 10 principal components\n",
        "    flattened_data = data.reshape((data.shape[0] * data.shape[1], data.shape[2]))\n",
        "    reduced_data = pca.fit_transform(flattened_data)\n",
        "    preprocessed_data = reduced_data.reshape((data.shape[0], data.shape[1], pca.n_components))\n",
        "    return preprocessed_data"
      ],
      "metadata": {
        "id": "KKGdFVR76Jwh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load HSI data and annotations\n",
        "HSI_data = []\n",
        "HSI_anno = []\n",
        "\n",
        "# Determine the maximum size along axis 1 for both data and annotations\n",
        "max_size_axis_1_data = 0\n",
        "max_size_axis_1_anno = 0\n",
        "\n",
        "for img_name in IMAGES:\n",
        "    data, _ = get_data(img_name)\n",
        "    HSI_data.append(data)\n",
        "    HSI_anno.append(np.ones(data.shape[:2]))  # Placeholder, replace with actual annotations\n",
        "\n",
        "    # Update max size along axis 1 for data if needed\n",
        "    max_size_axis_1_data = max(max_size_axis_1_data, data.shape[1])\n",
        "\n",
        "    # Update max size along axis 1 for annotations if needed\n",
        "    max_size_axis_1_anno = max(max_size_axis_1_anno, HSI_anno[-1].shape[1])\n",
        "\n",
        "# Pad the arrays along axis 1 to have the same size for both data and annotations\n",
        "HSI_data_padded = [np.pad(data, ((0, 0), (0, max_size_axis_1_data - data.shape[1]), (0, 0)), mode='constant') for data in HSI_data]\n",
        "HSI_anno_padded = [np.pad(anno, ((0, 0), (0, max_size_axis_1_anno - anno.shape[1])), mode='constant') for anno in HSI_anno]\n",
        "\n",
        "# Concatenate the padded arrays\n",
        "HSI_data = np.concatenate(HSI_data_padded, axis=0)\n",
        "HSI_anno = np.concatenate(HSI_anno_padded, axis=0)\n"
      ],
      "metadata": {
        "id": "HMVkvYA8639A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HSI_data_preprocessed = preprocess_data(HSI_data)"
      ],
      "metadata": {
        "id": "j2LH7IX8907b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(HSI_data_preprocessed, HSI_anno, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "uLQxrM9966yG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9qwTUwX7AFE",
        "outputId": "cd69f78a-8bcf-49ca-d0fc-20987981c433"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4889, 696, 10)\n",
            "y_train shape: (4889, 696)\n",
            "X_test shape: (2096, 696, 10)\n",
            "y_test shape: (2096, 696)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Assuming X_train and X_test are the preprocessed data\n",
        "# and y_train, y_test are binary labels\n",
        "\n",
        "# Reshape input data to (height, width, channels)\n",
        "input_shape = (696, 10, 1)\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Assuming y_train and y_test are originally (batch_size, 696)\n",
        "# Reshape them to (batch_size, 1)\n",
        "y_train_binary = y_train[:, :1]\n",
        "y_test_binary = y_test[:, :1]\n",
        "\n",
        "# Compile and train the CNN model\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train_binary, epochs=5, batch_size=128, validation_data=(X_test, y_test_binary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQosbrn4ijkf",
        "outputId": "478b317b-9cce-4a3b-e1ac-a3b470661bb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "39/39 [==============================] - 3s 23ms/step - loss: 0.0141 - accuracy: 0.9998 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 1s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 1s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 1s 14ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 1s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e97475a2ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Assuming X_train and X_test are your preprocessed data\n",
        "# and y_train_binary, y_test_binary are your binary labels\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(units=128, input_shape=(696, 10)))  # Adjust units and input shape as needed\n",
        "lstm_model.add(Dense(units=1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile and train the LSTM model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train_binary, epochs=5, batch_size=128, validation_data=(X_test, y_test_binary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJj7OAzwpXoQ",
        "outputId": "c895e173-409b-4803-bd0c-2ad5d69df5ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "39/39 [==============================] - 6s 62ms/step - loss: 0.1037 - accuracy: 0.9771 - val_loss: 9.9345e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 5.4647e-04 - accuracy: 1.0000 - val_loss: 3.0050e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 2.1837e-04 - accuracy: 1.0000 - val_loss: 1.5460e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.2401e-04 - accuracy: 1.0000 - val_loss: 9.7779e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 2s 48ms/step - loss: 8.3150e-05 - accuracy: 1.0000 - val_loss: 6.9637e-05 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e976c87be20>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models\n",
        "cnn_results = cnn_model.evaluate(X_test, y_test)\n",
        "lstm_results = lstm_model.evaluate(X_test, y_test)\n",
        "\n",
        "# Display evaluation metrics\n",
        "print(\"\\nCNN Evaluation:\")\n",
        "print(f\"Loss: {cnn_results[0]}\")\n",
        "print(f\"Accuracy: {cnn_results[1]}\")\n",
        "\n",
        "print(\"\\nLSTM Evaluation:\")\n",
        "print(f\"Loss: {lstm_results[0]}\")\n",
        "print(f\"Accuracy: {lstm_results[1]}\")"
      ],
      "metadata": {
        "id": "mqz1mw4F7u-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a9d8b9-1c0b-486b-da00-341164ae4d81"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66/66 [==============================] - 0s 3ms/step - loss: 0.9817 - accuracy: 0.9937\n",
            "66/66 [==============================] - 1s 12ms/step - loss: 0.0824 - accuracy: 0.9937\n",
            "\n",
            "CNN Evaluation:\n",
            "Loss: 0.9816523194313049\n",
            "Accuracy: 0.993693470954895\n",
            "\n",
            "LSTM Evaluation:\n",
            "Loss: 0.08241873979568481\n",
            "Accuracy: 0.993693470954895\n"
          ]
        }
      ]
    }
  ]
}